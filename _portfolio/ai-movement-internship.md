---
title: "AI Research Intern - Multi-view Breast Cancer Detection"
excerpt: "Advanced Deep Learning for Multi-View Structural Reasoning in Mammographic Analysis using Anatomy-aware Graph Networks, achieving 78.4%‚Äì92.5% Recall@[0.5,4.0]FPI"
collection: portfolio
permalink: /portfolio/ai-movement-internship/
date: 2024-08-01
venue: 'AiMovement/UM6P, Rabat, Morocco'
---

<div align="center">

<p><strong>Research Period:</strong> √ât√© 2024</p>

<p><strong>Institution:</strong><br>
<a href="https://www.aimovement.ma/">International Center for Artificial Intelligence of Morocco (AiMovement)</a><br>
<a href="https://www.um6p.ma/">Mohammed VI Polytechnic University (UM6P)</a>, Rabat, Morocco</p>

<p><strong>Contexte:</strong><br>
Premi√®re incursion du laboratoire dans l'imagerie m√©dicale ‚Ä¢ Impl√©mentation du papier "Act Like a Radiologist"</p>

<h3>üìã Table of Contents</h3>
<p>
  <a href="#-introduction">üìñ Introduction</a> ‚Ä¢
  <a href="#-objectives">üéØ Objectives</a> ‚Ä¢
  <a href="#Ô∏è-methods">‚öôÔ∏è Methods</a> ‚Ä¢
  <a href="#-results">üìä Results</a> ‚Ä¢
  <a href="#-discussion">üí¨ Discussion</a> ‚Ä¢
  <a href="#-references">üîó References</a>
</p>
</div>

---

## üìñ Introduction

**Contexte m√©dical**: Le cancer du sein est la pathologie n√©oplasique la plus r√©pandue chez les femmes, repr√©sentant environ 2,3 millions de nouveaux cas en 2022. La mammographie demeure l'outil de d√©pistage de r√©f√©rence, mais l'analyse traditionnelle mono-vue limite souvent la d√©tection pr√©coce, particuli√®rement dans les tissus mammaires denses.

**Approche clinique**: Les radiologues analysent naturellement plusieurs vues mammographiques simultan√©ment (cr√¢nio-caudale CC et m√©dio-lat√©rale oblique MLO) pour am√©liorer la pr√©cision diagnostique. Cette capacit√© de raisonnement multi-vue constitue un avantage diagnostique majeur.

**Innovation technologique**: Ce stage de recherche s'est concentr√© sur l'impl√©mentation et le raffinement du papier "Act Like a Radiologist", choisŸä pour son √©quilibre entre exigences de ressources et performances, ainsi que l'absence de code open-source disponible. Notre approche d√©veloppe un syst√®me de fusion multi-vue sophistiqu√© utilisant les **Anatomy-aware Graph Networks (AGN)** qui √©mulent les patterns d'interpr√©tation radiologique.

---

## üéØ Objectives

1. **R√©viser, impl√©menter et affiner** les m√©thodes de pointe pour la d√©tection mono-vue et multi-vue du cancer du sein
2. **D√©velopper un pipeline de pr√©processing robuste** pour l'identification de donn√©es et de landmarks anatomiques
3. **Impl√©menter l'architecture AGN** avec Bipartite Graph Network (BGN) pour les correspondances intra-vue et Inception Graph Network (IGN) pour la sym√©trie bilat√©rale
4. **Optimiser les performances** sous contraintes de donn√©es limit√©es avec approche d'entra√Ænement en deux √©tapes
5. **√âtablir des benchmarks comparatifs** contre les frameworks √©tablis (MaskRCNN, DETR, YOLO) sur le dataset CBIS-DDSM

---

## ‚öôÔ∏è Methods

### Architecture "Act Like a Radiologist" 

L'approche radiologique standard pour l'analyse mammographique implique:
1. **Analyse de vue individuelle** pour chaque projection mammographique
2. **Corr√©lation inter-vues** pour identifier les l√©sions correspondantes  
3. **Fusion multi-vue** pour la d√©cision diagnostique finale ‚Üê *R√©seaux de Neurones Graphiques appliqu√©s ici*

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/AGN.png" alt="AGN Architecture" width="70%">
  <p><em>Architecture g√©n√©rale AGN avec BGN et IGN</em></p>
</div>

### Anatomy-aware Graph Neural Network (AGN)

Notre impl√©mentation s'appuie sur l'architecture AGN qui fonctionne en imitant la capacit√© de raisonnement naturel que les radiologues appliquent lors du diagnostic:

**Composants cl√©s:**
- **Bipartite Graph Network (BGN)**: Mod√©lise les correspondances entre vues ipsilat√©rales (CC et MLO du m√™me sein)
- **Inception Graph Network (IGN)**: Exploite la sym√©trie bilat√©rale entre les seins gauche et droit
- **Pseudo-landmarks**: Points de r√©f√©rence anatomiquement coh√©rents (mamelon, muscle pectoral, contour mammaire)
- **Fusion par attention**: M√©canisme r√©siduel pour pr√©server et augmenter les caract√©ristiques

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/maskrcnn_adaptation.png" alt="MaskRCNN Adaptation" width="75%">
  <p><em>Adaptation MaskRCNN pour l'analyse multi-vue du cancer du sein</em></p>
</div>

### Preprocessing Pipeline et Extraction d'√âl√©ments Structurels

#### Nettoyage des Donn√©es CBIS-DDSM
Le dataset CBIS-DDSM pr√©sente plusieurs d√©fis n√©cessitant un preprocessing approfondi:
- **Images miroir**: 26.7% du dataset n√©cessitait une correction d'orientation
- **Artefacts et bordures**: Suppression par seuillage adaptatif et cropping bas√© coordonn√©es
- **Fichiers corrompus**: D√©tection et correction des ROI remplac√©s par des masques binaires
- **Incoh√©rences de r√©solution**: Normalisation des dimensions entre images et masques

#### Extraction de Landmarks Anatomiques
**D√©tection du contour mammaire**: Seuillage OTSU avec offset ajust√© $t_{adjusted} = t^* - \alpha$ et lissage B-spline

**D√©tection du muscle pectoral**: 
- Vues CC: Ligne verticale approximative √† l'√©tendue m√©diale
- Vues MLO: Approche multi-√©tapes avec CLAHE, seuillage combin√©, d√©tection de contours Canny, et transform√©e de Hough probabiliste

**D√©tection du mamelon**:
- Vues CC: Point le plus lat√©ral du contour mammaire
- Vues MLO: Analyse de courbure avec $\kappa(u) = \frac{x'(u)y''(u) - y'(u)x''(u)}{(x'(u)^2 + y'(u)^2)^{3/2}}$

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/pseudo.png" alt="Pseudo-landmarks" width="40%">
  <p><em>G√©n√©ration de pseudo-landmarks: (a) Vue CC, (b) Vue MLO</em></p>
</div>

### Impl√©mentation Technique D√©taill√©e

#### Architecture MaskRCNN Baseline
- **Backbone**: ResNet-50 + Feature Pyramid Network (FPN) pour extraction multi-√©chelle
- **RPN**: Anchors optimis√©s par K-means - 5 tailles [4,7,8,10,12] et 3 ratios [1.5,2.5,3.6]
- **ROI Align**: Configuration 7√ó7 pour d√©tection, 14√ó14 pour segmentation
- **T√™tes de d√©tection/masque**: Classification binaire (masse/arri√®re-plan) + r√©gression de bo√Ætes

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/maskrcnn_architecture.png" alt="MaskRCNN Architecture" width="80%">
  <p><em>Architecture MaskRCNN compl√®te avec backbone ResNet-50+FPN</em></p>
</div>

#### Strat√©gie d'Entra√Ænement en 3 √âtapes
Pour r√©soudre les probl√®mes de surajustement avec donn√©es limit√©es:
1. **√âtape 1**: Backbone gel√©, entra√Ænement t√™tes de d√©tection uniquement (√©poques 0-20)
2. **√âtape 2**: D√©gel partiel couches backbone de haut niveau (√©poques 20-40)
3. **√âtape 3**: Fine-tuning end-to-end avec r√©gularisation renforc√©e (√©poques 40-60)

#### Optimisations GPU et Augmentation
- **Augmentation probabiliste en ligne**: Albumentation avec flip horizontal, rotation, affine, distorsion √©lastique
- **Precision mixte**: Entra√Ænement FP16 pour optimisation m√©moire
- **Configuration SGD**: LR=0.002, momentum=0.9, decay=0.0001, scheduler step=15

---

## üõ†Ô∏è Configuration Exp√©rimentale

### Configuration Dataset CBIS-DDSM

#### Donn√©es d'Entra√Ænement
- **Dataset principal**: CBIS-DDSM avec 1,566 patients et 3,069 images mammographiques
- **Groupes tri-vues**: 111 groupes (87 entra√Ænement, 24 test) apr√®s filtrage patients ‚â•3 mammographies
- **Vues**: Cr√¢nio-caudale (CC) et m√©dio-lat√©rale oblique (MLO)
- **R√©solution**: 4084√ó3328 pixels, r√©solution 42.5-200 Œºm
- **D√©fi statistique**: Dataset d√©s√©quilibr√© masses, absence d'images enti√®rement saines

#### Algorithme de Groupement Multi-vue
Strat√©gie de groupement en trois cat√©gories: examin√©e, controlat√©rale, et auxiliaire
```python
# Algorithme de groupement tri-vue
for each patient p in P:
    if |P[p]| < 3: continue
    for each image i in P[p]:
        ve, se = View(i), Side(i)
        C = {j: View(j) = ve AND Side(j) != se}
        A = {j: View(j) != ve AND Side(j) = se}
        if C and A: create_triad(i, c, a)
```

### Infrastructure Technique

#### Configuration Mat√©rielle
- **GPU**: NVIDIA A100 40GB pour entra√Ænement AGRCNN
- **Optimisations**: Pr√©cision mixte automatique, gradient clipping
- **Temps d'inf√©rence**: MaskRCNN 79ms vs AGRCNN 432ms (5.5√ó plus lent)

#### Stack Logiciel
- **Framework**: PyTorch avec poids pr√©-entra√Æn√©s ImageNet
- **Preprocessing**: Modification architecture pour images niveaux de gris
- **√âvaluation**: Seuil IoU r√©duit √† 0.2 pour coh√©rence avec √©tudes comparatives

---

## üìä Results

### Comparaison Performance FROC

| Mod√®le | R@0.5FPI | R@1.0FPI | R@2.0FPI | R@3.0FPI | R@4.0FPI | Dataset |
|--------|----------|----------|----------|----------|----------|----------|
| **ALR MaskRCNN+FPN** | 76.0% | 82.5% | 88.7% | 90.8% | 91.4% | DDSM (2,620 img) |
| **Notre MaskRCNN+FPN** | 68.9% | 79.8% | 86.3% | 90.2% | 91.3% | CBIS-DDSM (1,560 img) |
| **Notre AGRCNN** | **78.4%** | **85.5%** | **90.1%** | **91.6%** | **92.5%** | CBIS-DDSM |

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/agn_froc.png" alt="FROC Comparison" width="60%">
  <p><em>Analyse FROC comparative: MaskRCNN, YOLO, DETR et AGRCNN sur CBIS-DDSM</em></p>
</div>

### Am√©liorations de Performance Cl√©s

**M√©triques principales:**
- **+9.5% d'am√©lioration** Recall@0.5FPI par rapport au baseline MaskRCNN
- **Performance sup√©rieure** malgr√© 40% de donn√©es en moins vs dataset DDSM original
- **Am√©lioration coh√©rente** sur tous les seuils FPI, particuli√®rement significative aux faibles FPI

### √âtudes d'Ablation Compl√®tes

**Analyse par composants:**

| M√©thode | R@0.5FPI | R@1.0FPI | R@2.0FPI | Notes |
|---------|----------|----------|----------|-------|
| **MaskRCNN (Baseline)** | 68.9% | 79.8% | 86.3% | D√©tection mono-vue |
| **+ BGN uniquement** | 72.1% | 81.5% | 87.8% | Correspondances ipsilat√©rales |
| **+ IGN uniquement** | 71.3% | 82.2% | 88.1% | Sym√©trie bilat√©rale |
| **+ AGN (fusion originale)** | 54.2% | 63.1% | 68.9% | M√©canisme d'attention destructif |
| **+ AGN (nos modifications)** | **78.4%** | **85.5%** | **90.1%** | **Connexions r√©siduelles** |

**Optimisation densit√© pseudo-landmarks:**
- **PL(13, 17)**: 76.8% recall@0.5FPI (configuration sparse)
- **PL(22, 26)**: **78.4%** recall@0.5FPI (densit√© optimale) ‚≠ê
- **PL(100, 105)**: 77.2% recall@0.5FPI (sur-param√©trisation)

**Strat√©gie mapping kNN:**
- **k=1 (Voronoi)**: 75.2% (voisin le plus proche uniquement)
- **k=3**: **78.4%** (contexte optimal) ‚≠ê  
- **k=5**: 77.8% (sur-lissage des caract√©ristiques)

### Training Evolution

<div align="center">
  <p><em>[Training curves showing loss evolution and metric improvements across epochs]</em></p>
</div>

### Qualitative Results

<div align="center">
  <p><em>[Sample detection results showing single-view vs multi-view predictions with confidence scores]</em></p>
</div>

---

## üí¨ Discussion

### Contributions Techniques Majeures

#### Solution Inspir√©e de ResNet
**Probl√®me identifi√©**: Le m√©canisme d'attention AGN original √©tait destructif, √©liminant compl√®tement les caract√©ristiques apprises du MaskRCNN avec $F_{enhanced} = \sigma(F_I \mathbf{w}_I) \odot F_e$ o√π les valeurs d'attention approchaient syst√©matiquement z√©ro.

**Notre solution r√©siduelle**:
```python
# Attention r√©siduelle avec pr√©servation des caract√©ristiques
ign_spatial_features = examined_features * (2.0 * ign_attention_map)
ign_spatial_features = ign_spatial_features + 0.2 * examined_features
```
Transformation de la plage d'attention de [0,1] √† [0.2,2.2] permettant suppression (attention < 0.5) ET augmentation (attention > 0.5).

<div align="center">
  <img src="https://raw.githubusercontent.com/ibouftini/ALR-portfolio/main/images/agn_results_2.png" alt="AGN Results" width="75%">
  <p><em>R√©sultats AGN apr√®s modifications: r√©duction arri√®re-plan/contour, am√©lioration r√©gion masse</em></p>
</div>

#### Entra√Ænement Progressif en 2 √âtapes
1. **√âtape 1**: Pr√©-entra√Ænement MaskRCNN sur donn√©es mammographiques compl√®tes
2. **√âtape 2**: Int√©gration AGN avec poids MaskRCNN gel√©s pour apprentissage relations graphiques

### Signification Clinique

- **Am√©lioration sensibilit√©**: D√©tection sup√©rieure de l√©sions subtiles manqu√©es par analyse mono-vue
- **R√©duction faux positifs**: Pr√©dictions robustes via consensus multi-vue  
- **Workflow inspir√© radiologue**: √âmulation des patterns diagnostiques d'experts

### D√©fis et Limitations

**Contrainte donn√©es limit√©es**: Seulement 111 groupes tri-vues disponibles
**Solution**: Strat√©gie d'entra√Ænement progressive + connexions r√©siduelles

**Complexit√© computationnelle**: Surco√ªt 5.5√ó en temps d'inf√©rence (432ms vs 79ms)
**Impact**: Acceptable pour pipelines de d√©pistage clinique o√π pr√©cision > vitesse

**Correspondances inter-vues**: D√©fi d'alignement l√©sions entre projections diff√©rentes
**Approche**: Pseudo-landmarks anatomiques + apprentissage correspondances par attention

### Directions de Recherche Future

**Extensions imm√©diates**:
- **Module de classification**: Diff√©renciation maligne/b√©nigne des masses d√©tect√©es
- **Optimisation temps d'inf√©rence**: R√©duction surco√ªt computationnel preprocessing
- **Datasets plus larges**: Extension validation sur OPTIMAM, EMBED

**Perspectives ambitieuses**: 
**Limitation fondamentale 2D**: Toutes les techniques multi-vue 2D tentent d'inf√©rer relations 3D depuis projections 2D, o√π la superposition tissulaire masque la distribution r√©elle des l√©sions.

**Vision 3D future**: 
- **Tomosynth√®se mammaire digitale**: Exploitation information 3D native
- **Reconstruction volum√©trique**: Algorithmes synth√©tisant descriptions 3D depuis projections mammographiques conventionnelles
- **R√©solution ambigu√Øt√©s spatiales**: Diff√©renciation masses r√©elles vs tissus normaux superpos√©s

Cette limitation pointe vers une direction future plus ambitieuse: d√©velopper de v√©ritables capacit√©s d'analyse 3D pour r√©soudre la superposition tissulaire et permettre une d√©tection confiante.

---

## üîó References

[1] [Author et al. "Act Like a Radiologist: Towards Reliable Multi-view Correspondence Reasoning for Mammogram Mass Detection"](https://arxiv.org/placeholder)

[2] [Shen, L., et al. (2019). "Deep Learning to Improve Breast Cancer Detection on Screening Mammography"](https://www.nature.com/articles/s41598-019-48995-4)

[3] [Kipf, T. N., & Welling, M. (2016). "Semi-Supervised Classification with Graph Convolutional Networks"](https://arxiv.org/abs/1609.02907)

[4] [Veliƒçkoviƒá, P., et al. (2017). "Graph Attention Networks"](https://arxiv.org/abs/1710.10903)

[5] [He, K., et al. (2017). "Mask R-CNN"](https://arxiv.org/abs/1703.06870)

---